# devops-pipeline-kit

# Projet ELT - Car Prices ğŸš—

Ce projet met en place une **pipeline ELT** complÃ¨te (Extract, Load, Transform) dÃ©ployÃ©e via **Docker Compose**, permettant dâ€™extraire un fichier CSV, de le charger dans **PostgreSQL**, et de planifier ce processus avec **n8n**. Le tout est encapsulÃ© dans des conteneurs Docker pour faciliter la portabilitÃ©.

---

## Stack utilisÃ©es

| Technologie        | RÃ´le dans le projet                                               |
| ------------------ | ----------------------------------------------------------------- |
| **Python**         | Scripts dâ€™extraction/chargement via `pandas` + API avec `FastAPI` |
| **FastAPI**        | API REST dÃ©clenchant l'import du CSV dans PostgreSQL              |
| **pandas**         | Lecture/chargement du CSV dans un DataFrame                       |
| **SQLAlchemy**     | Connexion entre Python et PostgreSQL, chargement avec `to_sql()`  |
| **PostgreSQL**     | Stockage des donnÃ©es structurÃ©es                                  |
| **pgAdmin**        | Interface graphique pour vÃ©rifier les tables                      |
| **n8n**            | Orchestration du dÃ©clenchement via HTTP POST                      |
| **Docker Compose** | Supervision et mise en rÃ©seau des conteneurs                      |

---

## ğŸ›  Installation

### 1. PrÃ©-requis

* Docker Desktop installÃ© et lancÃ©
* Compte GitHub (pour cloner le dÃ©pÃ´t)
* Ã‰diteur de texte

### 2. Cloner le projet

```bash
git clone https://github.com/REPO/nom-du-projet.git
cd nom-du-projet
```

### 3. Ajouter les fichiers nÃ©cessaires

* CrÃ©e un dossier `datasets` Ã  la racine
* Place un fichier `car_prices.csv` dedans

### 4. VÃ©rifie `.env`

CrÃ©er un fichier `.env` Ã  la racine avec :

```env
POSTGRES_PASSWORD=****
POSTGRES_DB=nom_serveur
API_URL=http://api
SYNC_INTERVAL=900
```

### 5. Lancer Docker Compose

```bash
docker compose up --build
```

Les services suivants seront lancÃ©s :

* `elt_postgres` â†’ base PostgreSQL
* `elt_pipeline` â†’ API FastAPI
* `elt_pgadmin` â†’ pgAdmin 
* `elt_n8n` â†’ n8n 

### 6. Connexion Ã  pgAdmin (facultatif)

* URL : [http://localhost:5050](http://localhost:5050)
* Email : [admin@admin.com](mailto:admin@admin.com)
* Mot de passe : admin
* Serveur : `elt_postgres` port `5432`

### 7. Utiliser n8n

* [http://localhost:5678](http://localhost:5678)
* CrÃ©e un `Schedule Trigger` â†’ `HTTP Request` vers `http://elt_pipeline:8000/load_csv`

---

## Structure du projet

```bash
â”œâ”€â”€ docker-compose.yml            # Orchestration des conteneurs
â”œâ”€â”€ .env                          # Variables dâ€™environnement
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ Dockerfile                # Image Python avec FastAPI
â”‚   â”œâ”€â”€ docker-entrypoint.sh      # EntrÃ©e du conteneur pipeline
â”‚   â”œâ”€â”€ state/                    # Logs CSV
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ main.py               # Serveur FastAPI + orchestration
â”‚       â””â”€â”€ load_csv.py           # Script dâ€™ingestion CSV
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ car_prices.csv            # DonnÃ©e source
```

---

## ğŸ” Fonctionnement de la pipeline

### Ã‰tape 1 : Extraction (E)

* Script : `load_csv.py`
* BibliothÃ¨que : `pandas`
* Lecture du fichier local `/datasets/car_prices.csv`

### Ã‰tape 2 : Chargement (L)

* Connexion via `SQLAlchemy`
* Chargement dans PostgreSQL (schÃ©ma `public`, table `car_prices`)

### Ã‰tape 3 : Transformation (T)

* (Non incluse ici mais faisable via SQL ou dbt si souhaitÃ©)

### Ã‰tape 4 : DÃ©clenchement manuel/automatisÃ©

* via n8n (`HTTP Request`)
* ou test local via Postman/cURL :

```bash
curl -X POST http://localhost:8000/load_csv
```

---

## âœ… VÃ©rifications et tests

### Logs

Accessible via :

```bash
GET http://localhost:8000/logs
```

### pgAdmin

* Connecte-toi au serveur `elt_postgres`
* Base : `elt_postgreso`
* Table : `public.car_prices`

---

## RÃ©sultat attendu

* Une table `car_prices` avec plus de 550k lignes chargÃ©e dans PostgreSQL
* Une API accessible pour recharger les donnÃ©es
* Un dÃ©clencheur visuel via n8n ou appel API

---

## ğŸ“Œ Commandes utiles

```bash
docker compose up --build          # Lancer tous les conteneurs
docker compose down                # Stopper les conteneurs
docker exec -it elt_postgres bash # AccÃ¨s Ã  la base Postgres
docker logs elt_pipeline           # Voir les logs du microservice
```

---

## Avantages de l'architecture 

* Projet Ã  double orchestration python gÃ¨re le workflow interne & n8n gÃ¨re la grande orchestration.
* ModularitÃ©  : chaque composant est indÃ©pendant (microservices).
* ReproductibilitÃ© : tout peut Ãªtre relancÃ© avec une seule commande docker compose up.
* ScalabilitÃ© : Enrichir facilement la pipeline avec dâ€™autres Ã©tapes ou traitements.

## AmÃ©liorations possibles

* ğŸ”¹Ajouter un nÅ“ud (n8n) PostgreSQL : pour permettre de vÃ©rifier que les donnÃ©es sont bien en base
* ğŸ”¹Ajout d'une notification automatique par mail, ou discord pour montrer que la pipeline sait notifier 
* ğŸ”¹Validation des donnÃ©es avant import
* ğŸ”¹Ajout de dbt ou Airflow pour pipeline avancÃ©
* ğŸ”¹Export vers un dashboard Looker Studio ou Power BI
